{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[0,0,1],\n",
    "                 [0,1,1],\n",
    "                 [1,0,1],\n",
    "                 [0,1,0],\n",
    "                 [1,0,0],\n",
    "                 [1,1,1],\n",
    "                 [0,0,0]\n",
    "                ])\n",
    "target = np.array([[0],[1],[1],[1],[1],[0],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, activator='sigmoid', n_input=3, n_hidden=4, n_output=1):\n",
    "        self.input = n_input\n",
    "        self.hidden_nodes = n_hidden\n",
    "        self.output_nodes = n_output\n",
    "        self.activator = activator\n",
    "        \n",
    "        np.random.seed(422)\n",
    "        \n",
    "        self.weights1 = np.random.randn(\n",
    "            self.input,\n",
    "            self.hidden_nodes\n",
    "        )\n",
    "        self.weights2 = np.random.randn(\n",
    "            self.hidden_nodes,\n",
    "            self.output_nodes\n",
    "        )\n",
    "        \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def sigmoidPrime(self, x):\n",
    "        return(x * (1 - x))\n",
    "    # Set up for multiple activation functions\n",
    "    def activate(self, x):\n",
    "        if self.activator == 'sigmoid':\n",
    "            return(self.sigmoid(x))\n",
    "    def grad_a(self, x):\n",
    "        if self.activator == 'sigmoid':\n",
    "            return(self.sigmoidPrime(x))\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        self.activated_hidden = self.activate(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.activate(self.output_sum)\n",
    "        return self.activated_output\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.grad_a(o)\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.grad_a(self.activated_hidden)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "\n",
    "    def train(self, X, y,):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn341 = NeuralNetwork(activator='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.25414929]         [0]\n",
      "[0.18817488]         [1]\n",
      "[0.23399115]         [1]\n",
      "[0.13493474]         [1]\n",
      "[0.163491]         [1]\n",
      "[0.18292325]         [0]\n",
      "[0.1766184]         [0]\n",
      "Loss: \n",
      " 0.40330880423665133\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.37271945]         [0]\n",
      "[0.32261701]         [1]\n",
      "[0.36002662]         [1]\n",
      "[0.21797027]         [1]\n",
      "[0.22952303]         [1]\n",
      "[0.33930026]         [0]\n",
      "[0.2390464]         [0]\n",
      "Loss: \n",
      " 0.3406866504491725\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.48602072]         [0]\n",
      "[0.46850524]         [1]\n",
      "[0.47844971]         [1]\n",
      "[0.32202024]         [1]\n",
      "[0.29984385]         [1]\n",
      "[0.50184867]         [0]\n",
      "[0.3084659]         [0]\n",
      "Loss: \n",
      " 0.29822800149170453\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.54649942]         [0]\n",
      "[0.55524031]         [1]\n",
      "[0.54452544]         [1]\n",
      "[0.40192222]         [1]\n",
      "[0.35156626]         [1]\n",
      "[0.59496214]         [0]\n",
      "[0.35800567]         [0]\n",
      "Loss: \n",
      " 0.28060588895473904\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.56897365]         [0]\n",
      "[0.59565141]         [1]\n",
      "[0.57542292]         [1]\n",
      "[0.45212635]         [1]\n",
      "[0.38659673]         [1]\n",
      "[0.64082764]         [0]\n",
      "[0.38698048]         [0]\n",
      "Loss: \n",
      " 0.2720482212306583\n",
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.12226501]         [0]\n",
      "[0.74190564]         [1]\n",
      "[0.76178431]         [1]\n",
      "[0.78558041]         [1]\n",
      "[0.80807638]         [1]\n",
      "[0.81558149]         [0]\n",
      "[0.16181939]         [0]\n",
      "Loss: \n",
      " 0.130353896402769\n",
      "+---------EPOCH 200---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.08024273]         [0]\n",
      "[0.70162286]         [1]\n",
      "[0.73246674]         [1]\n",
      "[0.80409438]         [1]\n",
      "[0.85797407]         [1]\n",
      "[0.73495999]         [0]\n",
      "[0.10304473]         [0]\n",
      "Loss: \n",
      " 0.11091094831651604\n",
      "+---------EPOCH 300---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.07033031]         [0]\n",
      "[0.67749515]         [1]\n",
      "[0.70957197]         [1]\n",
      "[0.83859894]         [1]\n",
      "[0.89263337]         [1]\n",
      "[0.67088015]         [0]\n",
      "[0.10180852]         [0]\n",
      "Loss: \n",
      " 0.09876102987741112\n",
      "+---------EPOCH 400---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.07919278]         [0]\n",
      "[0.74463354]         [1]\n",
      "[0.73262746]         [1]\n",
      "[0.80869786]         [1]\n",
      "[0.88982041]         [1]\n",
      "[0.51841082]         [0]\n",
      "[0.10707482]         [0]\n",
      "Loss: \n",
      " 0.067417492107151\n",
      "+---------EPOCH 500---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Predicted Output:   Actual Output: \n",
      "\n",
      "[0.04517481]         [0]\n",
      "[0.87284018]         [1]\n",
      "[0.87468224]         [1]\n",
      "[0.88559897]         [1]\n",
      "[0.93106259]         [1]\n",
      "[0.21248946]         [0]\n",
      "[0.1075895]         [0]\n",
      "Loss: \n",
      " 0.015497451106083953\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "y = target\n",
    "\n",
    "epochs = 500\n",
    "for i in range(epochs):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % int(epochs / 5) == 0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---' * 3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Predicted Output:   Actual Output: \\n') \n",
    "        for pr, ta in zip(nn341.feed_forward(X), y):\n",
    "            print(pr, \"       \", ta)\n",
    "        print('Loss: \\n', str(np.mean(np.square(y - nn341.feed_forward(X)))))\n",
    "    nn341.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#import gzip\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import struct as st\n",
    "\n",
    "#import os\n",
    "#import sys\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "# Config matplotlib for inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_loader:\n",
    "    \n",
    "    def __init__(self, abs_directory='/'):\n",
    "        self.abs_directory = abs_directory\n",
    "    \n",
    "    def load_data(self, fname):\n",
    "        datafile = open(str(self.abs_directory + fname), 'rb')\n",
    "        \n",
    "        #Skip to 4th byte to get size info\n",
    "        datafile.seek(4)\n",
    "        \n",
    "        num_arrays = st.unpack('>I',datafile.read(4))[0]\n",
    "        arr_rows = st.unpack('>I',datafile.read(4))[0]\n",
    "        arr_cols = st.unpack('>I',datafile.read(4))[0]\n",
    "        print('Number of images, Rows per image, Columns per image')\n",
    "        print(num_arrays, arr_rows, arr_cols)\n",
    "        \n",
    "        num_bytes_total = num_arrays * arr_rows * arr_cols\n",
    "                \n",
    "        data_array = 255 - (np.asarray(st.unpack('>'+'B'*num_bytes_total,\n",
    "                                                 datafile\n",
    "                                                 .read(num_bytes_total)))\n",
    "                            .reshape((num_arrays,(arr_rows*arr_cols)))\n",
    "                           )\n",
    "        \n",
    "        self.data_array = data_array\n",
    "        if len(self.data_array) > 1:\n",
    "            print('Successfully loaded {}'.format(fname))\n",
    "            print('Size: ', self.data_array.shape)\n",
    "        else:\n",
    "            print('There was an error')\n",
    "        \n",
    "    \n",
    "    def load_labels(self, fname):\n",
    "        labelfile = open(str(self.abs_directory + fname), 'rb')\n",
    "        labelfile.seek(4)\n",
    "        numL =  st.unpack('>I',labelfile.read(4))[0]\n",
    "        tl_array = np.asarray(st.unpack('>'+'B'*numL,labelfile.read(numL)))\n",
    "        self.labels_list = tl_array\n",
    "        if len(self.labels_list) > 1:\n",
    "            print('Successfully loaded {}'.format(fname))\n",
    "            print('Size: ', self.labels_list.shape)\n",
    "        else:\n",
    "            print('There was an error')\n",
    "        \n",
    "    def return_labels_data(self, lab_fname, data_fname):\n",
    "        self.load_data(data_fname)\n",
    "        self.load_labels(lab_fname)\n",
    "        return(self.data_array, self.labels_list)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images, Rows per image, Columns per image\n",
      "60000 28 28\n",
      "Successfully loaded train-images.idx3-ubyte\n",
      "Size:  (60000, 784)\n",
      "Successfully loaded train-labels.idx1-ubyte\n",
      "Size:  (60000,)\n"
     ]
    }
   ],
   "source": [
    "mnl = Mnist_loader(abs_directory=str('/home/joe2/Documents/LambdaSchool/unit4/sprint2/mnist_data/'))\n",
    "mndata, mnlabels = mnl.return_labels_data('train-labels.idx1-ubyte', 'train-images.idx3-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_number = 10099\n",
    "plt.imshow(mndata[sample_number], cmap=\"gray_r\")\n",
    "plt.axis('off')\n",
    "print(\"Image Label: \", mnlabels[sample_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_array[500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2:\n",
    "    def __init__(self, activator='sigmoid', n_input=3, n_hidden=4, n_output=1):\n",
    "        self.input = n_input\n",
    "        self.hidden_nodes = n_hidden\n",
    "        self.output_nodes = n_output\n",
    "        self.activator = activator\n",
    "        \n",
    "        np.random.seed(422)\n",
    "        \n",
    "        self.weights1 = np.random.randn(\n",
    "            self.input,\n",
    "            self.hidden_nodes\n",
    "        )\n",
    "        self.weights2 = np.random.randn(\n",
    "            self.hidden_nodes,\n",
    "            self.output_nodes\n",
    "        )\n",
    "        \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def sigmoidPrime(self, x):\n",
    "        return(x * (1 - x))\n",
    "    # Set up for multiple activation functions\n",
    "    def activate(self, x):\n",
    "        if self.activator == 'sigmoid':\n",
    "            return(self.sigmoid(x))\n",
    "    def grad_a(self, x):\n",
    "        if self.activator == 'sigmoid':\n",
    "            return(self.sigmoidPrime(x))\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        self.activated_hidden = self.activate(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.activate(self.output_sum)\n",
    "        return self.activated_output\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.grad_a(o)\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.grad_a(self.activated_hidden)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "\n",
    "    def train(self, X, y,):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "mNN = NeuralNetwork2(activator='sigmoid', n_input=784, n_hidden=100, n_output=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.98823529, 0.92941176, 0.92941176,\n",
       "       0.92941176, 0.50588235, 0.46666667, 0.31372549, 0.89803922,\n",
       "       0.34901961, 0.        , 0.03137255, 0.50196078, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.88235294, 0.85882353, 0.63137255, 0.39607843,\n",
       "       0.33333333, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "       0.00784314, 0.11764706, 0.3254902 , 0.00784314, 0.05098039,\n",
       "       0.23529412, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.80784314, 0.06666667,\n",
       "       0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "       0.00784314, 0.00784314, 0.00784314, 0.01568627, 0.63529412,\n",
       "       0.67843137, 0.67843137, 0.78039216, 0.84705882, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.92941176, 0.14117647, 0.00784314, 0.00784314,\n",
       "       0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.28627451,\n",
       "       0.03137255, 0.05490196, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.68627451, 0.38823529, 0.58039216, 0.00784314, 0.00784314,\n",
       "       0.19607843, 0.95686275, 1.        , 0.83137255, 0.39607843,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.94509804,\n",
       "       0.99607843, 0.39607843, 0.00784314, 0.64705882, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.45490196,\n",
       "       0.00784314, 0.25490196, 0.99215686, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.95686275, 0.25490196, 0.00784314,\n",
       "       0.7254902 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.8627451 , 0.05490196, 0.11764706, 0.37254902,\n",
       "       0.57647059, 0.99607843, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.68235294, 0.05882353, 0.00784314, 0.00784314, 0.53333333,\n",
       "       0.90196078, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.82352941,\n",
       "       0.27058824, 0.00784314, 0.00784314, 0.41176471, 0.89411765,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.9372549 , 0.63529412,\n",
       "       0.01176471, 0.00784314, 0.26666667, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.02352941, 0.00784314,\n",
       "       0.02352941, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.81960784, 0.49019608,\n",
       "       0.28235294, 0.00784314, 0.00784314, 0.18823529, 0.99215686,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.84705882,\n",
       "       0.41960784, 0.10196078, 0.00784314, 0.00784314, 0.00784314,\n",
       "       0.01960784, 0.28627451, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.90588235, 0.55294118, 0.13333333, 0.00784314, 0.00784314,\n",
       "       0.00784314, 0.00784314, 0.21176471, 0.69411765, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.90980392, 0.74117647, 0.16470588, 0.00784314,\n",
       "       0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.68235294,\n",
       "       0.99215686, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.92941176, 0.32941176, 0.14117647,\n",
       "       0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.23529412,\n",
       "       0.68627451, 0.96470588, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.78431373, 0.3254902 ,\n",
       "       0.11372549, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "       0.04313725, 0.47843137, 0.95686275, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.46666667, 0.00784314, 0.00784314, 0.00784314,\n",
       "       0.16862745, 0.47058824, 0.48235294, 0.9372549 , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mndata_norm = mndata / np.amax(mndata)\n",
    "mndata_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(mnlabels, (-1,1))[:4]\n",
    "encoded_mnlabels = []\n",
    "ldict = {0:[1,0,0,0,0,0,0,0,0,0],\n",
    "         1:[0,1,0,0,0,0,0,0,0,0],\n",
    "         2:[0,0,1,0,0,0,0,0,0,0],\n",
    "         3:[0,0,0,1,0,0,0,0,0,0],\n",
    "         4:[0,0,0,0,1,0,0,0,0,0],\n",
    "         5:[0,0,0,0,0,1,0,0,0,0],\n",
    "         6:[0,0,0,0,0,0,1,0,0,0],\n",
    "         7:[0,0,0,0,0,0,0,1,0,0],\n",
    "         8:[0,0,0,0,0,0,0,0,1,0],\n",
    "         9:[0,0,0,0,0,0,0,0,0,1]\n",
    "        }\n",
    "invert = lambda d: dict(zip(d.values(), d.keys()))\n",
    "for i in mnlabels:\n",
    "    encoded_mnlabels.append(ldict[i])\n",
    "mnnp_enc = np.asarray(encoded_mnlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnnp_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_back(lst):\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == 1:\n",
    "            return(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_back(mnnp_enc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "Loss: \n",
      " 0.9\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 1 0 0 0 0]\n",
      "0         5\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1 0 0 0 0 0 0 0 0 0]\n",
      "0         0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 1 0 0 0 0 0]\n",
      "0         4\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 0 0 1]\n",
      "0         9\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 1 0 0 0 0 0 0 0]\n",
      "0         2\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 1 0 0 0 0 0 0]\n",
      "0         3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 1 0 0 0 0 0]\n",
      "0         4\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 1 0 0 0 0 0 0]\n",
      "0         3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 1 0 0 0 0]\n",
      "0         5\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 1 0 0 0 0 0 0]\n",
      "0         3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 1 0 0 0]\n",
      "0         6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 1 0 0]\n",
      "0         7\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 1 0 0 0 0 0 0 0]\n",
      "0         2\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 0 1 0]\n",
      "0         8\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 1 0 0 0]\n",
      "0         6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 0 0 1]\n",
      "0         9\n",
      "Loss: \n",
      " 0.9\n",
      "+---------EPOCH 10---------+\n",
      "Input: \n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 1 0 0 0 0]\n",
      "0         5\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1 0 0 0 0 0 0 0 0 0]\n",
      "0         0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 1 0 0 0 0 0]\n",
      "0         4\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 0 0 1]\n",
      "0         9\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 1 0 0 0 0 0 0 0]\n",
      "0         2\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 1 0 0 0 0 0 0]\n",
      "0         3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 1 0 0 0 0 0]\n",
      "0         4\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 1 0 0 0 0 0 0]\n",
      "0         3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 1 0 0 0 0]\n",
      "0         5\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 1 0 0 0 0 0 0]\n",
      "0         3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 1 0 0 0]\n",
      "0         6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 1 0 0 0 0 0 0 0 0]\n",
      "0         1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 1 0 0]\n",
      "0         7\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 1 0 0 0 0 0 0 0]\n",
      "0         2\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 0 1 0]\n",
      "0         8\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 1 0 0 0]\n",
      "0         6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [0 0 0 0 0 0 0 0 0 1]\n",
      "0         9\n",
      "Loss: \n",
      " 0.9\n"
     ]
    }
   ],
   "source": [
    "X = mndata_norm\n",
    "# y = np.reshape(mnlabels, (-1,1))\n",
    "y = mnnp_enc\n",
    "\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    if (i+1 in [1,5,10]) or ((i+1) % int(epochs / 2) == 0):\n",
    "        \n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---' * 3 + '+')\n",
    "        print('Input: \\n', X[:20])\n",
    "#        print('Predicted Output:   Actual Output: \\n') \n",
    "#         for pr, ta in zip(mNN.feed_forward(X), y):\n",
    "#             print(convert_back(pr), \"       \", convert_back(ta))\n",
    "        if ((i+1 == epochs / 2) or (i+1 == epochs)):\n",
    "            for pr, ta in zip(mNN.feed_forward(X)[:20], y[:20]):\n",
    "                print(pr, ta)\n",
    "                print(convert_back(pr), \"       \", convert_back(ta))\n",
    "        print('Loss: \\n', str(np.mean(np.square(y - mNN.feed_forward(X)))))\n",
    "    mNN.train(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.0\n",
      "1 2.0\n",
      "2 2.0\n",
      "3 2.0\n",
      "4 2.0\n",
      "49 2.0\n",
      "99 2.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % int(epochs / 2) == 0):\n",
    "        print(i, (1 + 1 % (epochs/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
